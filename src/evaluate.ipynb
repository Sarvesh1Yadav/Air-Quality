"""# ***Classification***"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Split Data
X = df.drop(columns=["AQI", "AQI_Range", "state", "location"])
y = df["AQI_Range"]


X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Define Model
rf = RandomForestClassifier(random_state=42, warm_start=True)

# Optimized Hyperparameter Search Space
rf_params = {
    "n_estimators": [50, 100, 150],
    "max_depth": [10, 20, None],
    "min_samples_split": [2, 5]
}


rf_random = RandomizedSearchCV(
    rf,
    rf_params,
    n_iter=10,
    cv=3,
    scoring="accuracy",
    n_jobs=-1,
    verbose=2,
    random_state=42,
)

# Train Model
rf_random.fit(X_train, y_train)

# Best RF Model
best_rf = rf_random.best_estimator_
print("Best Parameters:", rf_random.best_params_)

df.head(2)

from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=70)
import matplotlib.pyplot as plt
import seaborn as sns

# Get feature importance
feature_importance = pd.Series(best_rf.feature_importances_, index=X_train.columns)
feature_importance = feature_importance.sort_values(ascending=False)

# Plot feature importance
plt.figure(figsize=(10,5))
sns.barplot(x=feature_importance, y=feature_importance.index)
plt.xlabel("Feature Importance Score")
plt.ylabel("Features")
plt.title("Feature Importance from Random Forest")
plt.show()

"""# Selecting best features for Classification"""

top_n = 10

# Get the top N important features
top_features = feature_importance.index[:top_n]

# Keep only these features in X_train and X_test
X_train_selected = X_train[top_features]
X_test_selected = X_test[top_features]

print(f"Selected Features: {list(top_features)}")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix



# Encode AQI Range (Convert categories to numbers)
label_encoder = LabelEncoder()
df["AQI_Range"] = label_encoder.fit_transform(df["AQI_Range"])

rf = RandomForestClassifier(random_state=42)
rf_params = {
    "n_estimators": [100, 200, 300],
    "max_depth": [10, 20, 30, None],
    "min_samples_split": [2, 5, 10]
}

rf_grid = RandomizedSearchCV(rf, rf_params, cv=5, scoring="accuracy", n_jobs=-1, verbose=2)
rf_grid.fit(X_train_selected, y_train)

# Best RF Model
best_rf = rf_grid.best_estimator_
y_pred_rf = best_rf.predict(X_test_selected)

# XGBoost Classifier #
xgb = XGBClassifier(use_label_encoder=False, eval_metric="mlogloss", random_state=42)
xgb_params = {
    "n_estimators": [100, 200],
    "max_depth": [3, 5, 7],
    "learning_rate": [0.01, 0.1, 0.2]
}

xgb_grid = GridSearchCV(xgb, xgb_params, cv=5, scoring="accuracy", n_jobs=-1, verbose=2)
xgb_grid.fit(X_train_selected, y_train)

# Best XGBoost Model
best_xgb = xgb_grid.best_estimator_
y_pred_xgb = best_xgb.predict(X_test_selected)

# Model Evaluation #
def evaluate_model(model_name, y_true, y_pred):
    print(f"ðŸ“Š {model_name} Performance:")
    print(f"Accuracy: {accuracy_score(y_true, y_pred):.4f}")
    print("Classification Report:\n", classification_report(y_true, y_pred))
    print("Confusion Matrix:\n", confusion_matrix(y_true, y_pred))
    print("-" * 50)

evaluate_model("Random Forest", y_test, y_pred_rf)
evaluate_model("XGBoost", y_test, y_pred_xgb)

def predict_aqi(model, input_features):
    input_df = pd.DataFrame([input_features], columns=X_train_selected.columns)

    predicted_class = model.predict(input_df)[0]

    predicted_label = label_encoder.inverse_transform([predicted_class])[0]

    return predicted_label

# Select a specific index from the test set
index = 9

# Get the actual AQI range
actual_aqi = label_encoder.inverse_transform([y_test.iloc[index]])[0]

# Get the predicted AQI range
predicted_aqi_rf = predict_aqi(best_rf, X_test_selected.iloc[index].tolist())
predicted_aqi_xgb = predict_aqi(best_xgb, X_test_selected.iloc[index].tolist())

print(f"\nðŸŽ¯ Sample Index: {index}")
print(f"âœ… Actual AQI Range: {actual_aqi}")
print(f"ðŸŸ¢ RF Predicted AQI Range: {predicted_aqi_rf}")
print(f"ðŸ”µ XGB Predicted AQI Range: {predicted_aqi_xgb}")

# Training accuracy
train_acc_rf = accuracy_score(y_train, best_rf.predict(X_train_selected))
test_acc_rf = accuracy_score(y_test, best_rf.predict(X_test_selected))

train_acc_xgb = accuracy_score(y_train, best_xgb.predict(X_train_selected))
test_acc_xgb = accuracy_score(y_test, best_xgb.predict(X_test_selected))

print("ðŸ“Š Random Forest:")
print(f"âœ… Training Accuracy: {train_acc_rf:.4f}")
print(f"âœ… Test Accuracy: {test_acc_rf:.4f}\n")

print("ðŸ“Š XGBoost:")
print(f"âœ… Training Accuracy: {train_acc_xgb:.4f}")
print(f"âœ… Test Accuracy: {test_acc_xgb:.4f}")

"""# ***Regression***"""

df['AQI_Range'].unique()

"""# Splitting the dataset into Dependent and Independent columns"""

df.columns

from sklearn.ensemble import RandomForestRegressor

X = df.drop(columns=["AQI", "AQI_Range", "state", "location"])
y = df["AQI_Range"]


# Split Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Random Forest Regressor
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Get feature importance
feature_importance = pd.Series(rf.feature_importances_, index=X_train.columns)
feature_importance = feature_importance.sort_values(ascending=False)

# Plot feature importance
plt.figure(figsize=(10, 5))
sns.barplot(x=feature_importance, y=feature_importance.index)
plt.xlabel("Feature Importance Score")
plt.ylabel("Features")
plt.title("Feature Importance from Random Forest (Regression)")
plt.show()

# Select Top N Features
top_n = 6
top_features = feature_importance.index[:top_n]

# Keep only these features
X_train_selected = X_train[top_features]
X_test_selected = X_test[top_features]

print(f"Selected Features: {list(top_features)}")

"""# Air Pollution Level Prediction(Regression)"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

lr = LinearRegression()
lr.fit(X_train_selected, y_train)
y_pred_lr = lr.predict(X_test_selected)

# Evaluation
print("Linear Regression:")
print("MAE:", mean_absolute_error(y_test, y_pred_lr))
print("MSE:", mean_squared_error(y_test, y_pred_lr))
print("R2 Score:", r2_score(y_test, y_pred_lr))

from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train_selected, y_train)
y_pred_rf = rf.predict(X_test_selected)

# Evaluation
print("Random Forest:")
print("MAE:", mean_absolute_error(y_test, y_pred_rf))
print("MSE:", mean_squared_error(y_test, y_pred_rf))
print("R2 Score:", r2_score(y_test, y_pred_rf))

from xgboost import XGBRegressor

xgb = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
xgb.fit(X_train_selected, y_train)
y_pred_xgb = xgb.predict(X_test_selected)

# Evaluation
print("XGBoost:")
print("MAE:", mean_absolute_error(y_test, y_pred_xgb))
print("MSE:", mean_squared_error(y_test, y_pred_xgb))
print("R2 Score:", r2_score(y_test, y_pred_xgb))

import numpy as np
import tensorflow

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from sklearn.preprocessing import StandardScaler

# Scale data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_selected)
X_test_scaled = scaler.transform(X_test_selected)

# Reshape for LSTM
X_train_lstm = np.reshape(X_train_scaled, (X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_test_lstm = np.reshape(X_test_scaled, (X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

# Define LSTM model
model = Sequential([
    LSTM(50, return_sequences=True, input_shape=(1, X_train_scaled.shape[1])),
    LSTM(50),
    Dense(1)
])

# Compile and train
model.compile(optimizer='adam', loss='mse')
model.fit(X_train_lstm, y_train, epochs=25, batch_size=32, verbose=1)

# Predict
y_pred_lstm = model.predict(X_test_lstm)
y_pred_lstm = y_pred_lstm.flatten()

# Evaluation Metrics
mse = mean_squared_error(y_test, y_pred_lstm)
mae = mean_absolute_error(y_test, y_pred_lstm)
r2 = r2_score(y_test, y_pred_lstm)

print(f"LSTM MSE: {mse}")
print(f"LSTM MAE: {mae}")
print(f"LSTM RÂ² Score: {r2}")

"""# Random Forest with Hyperparameter tuning"""

from sklearn.model_selection import RandomizedSearchCV
import numpy as np

# Define Random Forest model
rf = RandomForestRegressor(random_state=42)

# Define parameter grid for tuning
param_grid_rf = {
    'n_estimators': [50, 100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

# RandomizedSearchCV for quick tuning
rf_grid_search = RandomizedSearchCV(
    estimator=rf, param_distributions=param_grid_rf,
    n_iter=20, cv=5, scoring='r2', verbose=2, n_jobs=-1, random_state=42
)

# Fit on training data
rf_grid_search.fit(X_train_selected, y_train)

# Best parameters
print("Best RF Params:", rf_grid_search.best_params_)

# Train final model with best params
best_rf = rf_grid_search.best_estimator_
y_pred_rf = best_rf.predict(X_test_selected)

# Evaluate RF model
rf_mae = mean_absolute_error(y_test, y_pred_rf)
rf_mse = mean_squared_error(y_test, y_pred_rf)
rf_r2 = r2_score(y_test, y_pred_rf)

print(f"Random Forest - MAE: {rf_mae}, MSE: {rf_mse}, RÂ² Score: {rf_r2}")

"""# XGBoost with Hyperparameter tuning"""

from xgboost import XGBRegressor
from sklearn.model_selection import GridSearchCV

# Define XGBoost model
xgb = XGBRegressor(objective='reg:squarederror', random_state=42)

# Define parameter grid for tuning
param_grid_xgb = {
    'n_estimators': [50, 100, 200, 300],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'max_depth': [3, 5, 7, 10],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0]
}

# GridSearchCV for precise tuning
xgb_grid_search = RandomizedSearchCV(
    estimator=xgb, param_distributions=param_grid_xgb,
    cv=5, scoring='r2', verbose=2, n_jobs=-1
)

# Fit on training data
xgb_grid_search.fit(X_train_selected, y_train)

# Best parameters
print("Best XGB Params:", xgb_grid_search.best_params_)

# Train final model with best params
best_xgb = xgb_grid_search.best_estimator_
y_pred_xgb = best_xgb.predict(X_test_selected)

# Evaluate XGBoost model
xgb_mae = mean_absolute_error(y_test, y_pred_xgb)
xgb_mse = mean_squared_error(y_test, y_pred_xgb)
xgb_r2 = r2_score(y_test, y_pred_xgb)

print(f"XGBoost - MAE: {xgb_mae}, MSE: {xgb_mse}, RÂ² Score: {xgb_r2}")

